{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1fbf793",
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3cbe3281",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.0+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "791c2aba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\jayjo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling,DataCollatorWithPadding\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5bc542e1",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jayjo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yeah, Im in high school. What are my interests? Well, my friends say I'm obsessed with celebrities. The truth is, I only in love with ONE! Theo James! I know his age, address, full name, where he lives, and where his entire family lives! I mean we're basically married. If you ever get to meet his family, you will love them!\n"
          ]
        }
      ],
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "def summarize_text(text, max_length=100):\n",
        "    # Load the pre-trained model and tokenizer\n",
        "    model_name = \"facebook/bart-large-cnn\"\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Prepare the input text\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "    # Generate summary\n",
        "    summary_ids = model.generate(input_ids, max_length=max_length, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode and return the summary\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Example usage\n",
        "text_to_summarize = \"Yeah, Im in high school. (pause) What are my interests? Well, my friends say I'm obsessed with celebrities. I just want to say to them, Look you little two-timing molded fruit cakes, I am NOT obsessed with celebrities! The truth is, I only in love with ONE! Theo James! I know his age, address, full name, where he lives, and where his entire family lives! I mean we're basically married. If you ever get to meet his family, you will love them! I sure will! I plan on paying them a little visit. You know, just to ask them a few questions like where's the nearest hardware store, oh and if Theo has any cameras at his house. That is the basic questions you ask your husband's parents, right? Yeah, I know this flight to England costs a lot of money, but he is worth it, anything for my hubby! I’m sure that he has gotten the hundreds of letters that I sent. He’s just too busy to write me back. Oh, I know he will be so excited to see me, well, when he regains consciousness anyway! What's that? You are calling the flight attendant to call the police? Oh, don't worry! He totally knows I am coming. I gave him a little call the other day. I guess he thought I was some obsessed teen off the street, but I am SO not obsessed! What? you think I am obsessed too? No ma'am! I am in LOVE! Anyway, here we are! I am so excited! Wish me luck!\"\n",
        "summary = summarize_text(text_to_summarize)\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "df0b19b6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Science Fiction: In a distant future, a group of scientists are trying to figure out how to make a living. They're trying to figure out how to make a living by using the power of the universe to create a new kind of life.\n",
            "\n",
            "The Science Fiction: In a distant future, a group of scientists are trying to figure out how to make a living. They're trying to figure out how to make a living by using the power of the universe to create a new kind of life\n"
          ]
        }
      ],
      "source": [
        "def generate_text(genre, prompt, max_length=100):\n",
        "    # Load the pre-trained model and tokenizer\n",
        "    model_name = \"gpt2\"\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Prepare the input text\n",
        "    input_text = f\"{genre}: {prompt}\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # Generate text\n",
        "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Example usage\n",
        "genre = \"Science Fiction\"\n",
        "prompt = \"In a distant future,\"\n",
        "generated_text = generate_text(genre, prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b36246fd",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fiction: Write a short story about time travel.\n",
            "\n",
            "The Story of Time Travel\n",
            "\n",
            "The story of time travel is a story about time travel. It's a story about time travel. It's a story about time travel. It's a story about time travel. It's a story about time travel. It's a story about time travel. It's a story about time travel. It's a story about time travel. It's a story about time travel. It's a story\n"
          ]
        }
      ],
      "source": [
        "def create_prompt(task, input_text):\n",
        "    if task == \"story\":\n",
        "        return f\"Write a short story about {input_text}.\"\n",
        "    elif task == \"explanation\":\n",
        "        return f\"Explain the concept of {input_text} in simple terms.\"\n",
        "    elif task == \"question\":\n",
        "        return f\"What are the implications of {input_text}?\"\n",
        "    else:\n",
        "        return f\"Generate text for: {input_text}\"\n",
        "\n",
        "# Example usage\n",
        "task = \"story\"\n",
        "input_text = \"time travel\"\n",
        "prompt = create_prompt(task, input_text)\n",
        "generated_story = generate_text(\"Fiction\", prompt)\n",
        "print(generated_story)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fcb5e511",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated Text: Le temps est beau aujourd'hui.\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the T5 model and tokenizer\n",
        "model_name = \"t5-small\"  # You can also try 't5-base' or 't5-large'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def translate(text, source_lang=\"English\", target_lang=\"French\"):\n",
        "    # Create the translation prompt for T5\n",
        "    prompt = f\"translate {source_lang} to {target_lang}: {text}\"\n",
        "    \n",
        "    # Tokenize the input text\n",
        "    input_tokens = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "    \n",
        "    # Generate translation\n",
        "    output_tokens = model.generate(**input_tokens, max_length=50)\n",
        "    \n",
        "    # Decode and return the translation\n",
        "    return tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "# Test the translation function\n",
        "text = \"The weather is beautiful today.\"\n",
        "print(\"Translated Text:\", translate(text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "29525255",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Style: Shakespearean\n",
            "Rewrite the following sentence in a Shakespearean style:\n",
            "\n",
            "The sky was blue and the sun was shining.\n",
            "\n",
            "\n",
            "So I went to sleep.\n",
            "\n",
            "\n",
            "But I woke up after a night without sleep.\n",
            "\n",
            "\n",
            "I saw the clouds.\n",
            "\n",
            "Style: casual\n",
            "Rewrite the following sentence in a casual style:\n",
            "\n",
            "The sky was blue and the sun was shining.\n",
            "\n",
            "\n",
            "A picture can tell a story.\n",
            "\n",
            "A picture can change the future.\n",
            "\n",
            "A picture can make life better.\n",
            "\n",
            "Style: formal\n",
            "Rewrite the following sentence in a formal style:\n",
            "\n",
            "The sky was blue and the sun was shining.\n",
            "\n",
            "\n",
            "In the same way that an image can be processed into form, it can be applied to any object. It can be applied to\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load the GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(\"cuda\")\n",
        "\n",
        "def rewrite_in_style(prompt, style):\n",
        "    # Modify the input prompt to reflect the desired style\n",
        "    styled_prompt = f\"Rewrite the following sentence in a {style} style:\\n\\n{prompt}\\n\\n\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer.encode(styled_prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Move the input to GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = inputs.to(\"cuda\")\n",
        "\n",
        "    # Generate text with GPT-2\n",
        "    outputs = model.generate(\n",
        "        inputs, \n",
        "        max_length=50,        # Increase the output length for more content\n",
        "        num_return_sequences=1, \n",
        "        temperature=0.9,       # Higher temperature for more variety\n",
        "        top_k=50,              # Top-k sampling\n",
        "        top_p=0.95,            # Nucleus sampling\n",
        "        do_sample = True, \n",
        "        pad_token_id=tokenizer.eos_token_id  # Use EOS token for padding\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    rewritten_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    return rewritten_text\n",
        "\n",
        "# Example usage with a single sentence\n",
        "prompt = \"The sky was blue and the sun was shining.\"\n",
        "styles = [\"Shakespearean\", \"casual\", \"formal\"]\n",
        "\n",
        "for style in styles:\n",
        "    print(f\"Style: {style}\\n{rewrite_in_style(prompt, style)}\\n\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
